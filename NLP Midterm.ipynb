{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Introducion\n",
    "1. Domain-specific area\n",
    "\n",
    "\tSentiment analysis is a combination of natural language processing, text analysis and computational linguistics used to study subjective information. The data source we chose for this project is a corpus of reviews of hotels which could be either true or false. Since our aim is to accurately process sentiments of text, part of the project will involve cleaning the data of the false reviews. Sentiment analysis has many applications across various fields, e.g., marketing, customer service and clinical medicine that can span areas such as reviews/surveys, social media applications and health care. Sentiment analysis can come in several forms some of which include subjectivity/objectivity identification, feature/aspect based identification and intensity ranking of a given sentiment/emotion. In this project, we’ve chosen to undergo polarity classification which can be described as one of the main subtasks of sentiment analysis/opinion mining. Perhaps one of the biggest industries in which this work can contribute would be social media. Companies that wish to market their products place massive importance upon things such as reviews, ratings and recommendations in order to navigate the rapidly shifting landscape of modern trends. Research into areas such as sentiment analysis (in this case polarity classification) could have potentially limitless value in the modern world of social media. \n",
    "\n",
    "\tIn its rawest form, the data source we’ve chosen is a csv file containing five columns. The columns include deceptive (meaning a true or false opinion), hotel, polarity (positive or negative opinion), source and text. The only relevant fields in this investigation are polarity, text and to some degree deceptive. As mentioned before, the false opinions will need to be cleaned from the data because we’re trying to mine true opinions from text using sentiment analysis in this project. This clearly has direct applications (although not exclusive applications) to the hotel industry. Another area in which this research has applications is amongst recommender systems. Recommender systems are important to many industries such as streaming services (e.g. Netflix, Hulu, HBO, etc.), social networking services and e-commerce websites (e.g. Amazon). This area of research is especially important to social networking services and e-commerce websites since user-generated text can provide highly valuable data about user’s opinions. The area of sentiment analysis (and particularly polarity classification) seems to have unlimited potential in the increasingly technocentric society of the modern age. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dataset\n",
    "\n",
    "\tThe objective of this research project is to utilize data from a hotel review corpus in order to create a text classifier utilizing sentiment analysis. The data source was a csv file found on Kaggle named Deceptive Opinion Scam Corpus. The corpus is composed of reviews of 20 hotels in Chicago and includes 400 true positive reviews, 400 false positive reviews, 400 true negative reviews and 400 false negative reviews. For the purpose of this research we will be removing all the positive reviews as that is not within the scope of this research. Therefore, the only significant fields in the data set include the text column composed of strings containing the customer reviews, the polarity column containing a string stating that the review is either positive or negative and the deceptive column containing a string stating that the review is either truthful or deceptive. The data source is roughly 1 MB in size. \n",
    "\t\n",
    "\tA significant amount of preprocessing was necessary for this dataset. The first processing we did involved eliminating the deceptive rows from the dataset. Next, we checked to see if there were any null data elements using the isnull() function. We then proceeded to whittle the dataframe down to just two columns comprising text and polarity. The sentiment analysis we planned to incorporate involved logistic regression and a confusion matrix for our evaluation metric. Since training sets which are overrepresented by one category or another can lead to overtraining we then checked the count of positive and negative reviews. Luckily, there were an even number of positive and negative reviews at this point meaning there wouldn’t be need to selectively reduce the population size in order to prevent overtraining. The next step was to remove from the text any punctuation as that was irrelevant to our investigation. Then we converted all the text to lowercase and utilized the NLTK stopwords library to exclude stopwords which are essentially filler words in English that humans use but only serve to confuse/obfuscate the task at hand for a machine. The last thing to do was convert the polarity column from strings to numbers as strings wouldn’t be useful in our logistic regression. Therefore, we converted all the positive sentiments in the polarity column to a 1 and the negative sentiments to a 0, temporarily appended this column to the dataframe and then redefined the dataframe to only include the columns containing the cleaned text and the polarity converted to binary. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Objectives\n",
    "\n",
    "\tThe objective of this project is to run a logistic regression on the dataset in order to predict positive and negative sentiments. The reason for this is the dataset is fundamentally binary in nature and therefore lends itself to this form of analysis. We will then utilize scikit-learn on the regression to create a confusion matrix to analyze the results. The fundamental objective of this project is to create a text classifier capable of lending itself to various fields of industry in order to improve customer-supplier relations/communication. \n",
    "\n",
    "\tThis research can contribute to many of the areas of industry stated previously. Business owners/employers of any of the areas stated earlier looking for customer feedback in order to improve their business model and increase production and customer satisfaction could utilize this research. In the long term, the entire profession of marketing could essentially be removed using this research (this paper doesn’t aim to discuss the potential ethical repercussions of its objectives). It’s become increasingly more pervasive for machines to take on the role of humans and there’s little doubt that if the field of sentiment analysis continues to grow then the role of marketers will continually diminish over time. Even as this paper is written, grammatical/spelling errors are automatically brought to the writer's attention and corrected. There can be little doubt that as this field continues to grow exponentially, the field of marketing will continue to become more specialized and niche. Only the marketers most capable of analyzing/executing research metadata will remain in the long term. Indubitably, only highly specialized fields and exceptional expertise will remain unaffected by the increase in AI/ML in the years to come. \n",
    "\n",
    "\tOne other area where this research may have contributions is where sentiments aren’t explicitly stated. Spheres in which authors explicitly express their opinions has been the main subject of research amongst sentiment analysis historically. But with the increased sophistication of this research, areas in which sentiment was highly implicit will become more accessible to this branch of natural language processing. For example, one domain in which sentiments are usually implicitly, rather than explicitly, expressed is in news articles. Journalists are expected to maintain a certain level of journalistic objectivity/integrity which disallows them from expressing direct opinions. The implicit sentiments expressed in areas such as journalism have recently become attainable to the methods used in sentiment analysis. This work aims to aid and contribute to this area of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluation Methodology\n",
    "\n",
    "\tFor the purpose of this experiment, we will be undergoing logistic regression on a dataset in order to evaluate sentiment. This methodology of evaluation automatically lends itself to a confusion matrix. Therefore, for our evaluation metric we will compare and contrast precision, recall, accuracy, and F-measure. The reason why we’ve chosen logistic regression to analyze the data is due to its ease of use, interpretation, implementation and efficiency in training. It’s the view of the author that any single metric isn’t sufficient to fully summarize a dataset and, therefore, since confusion matrices lend themselves to a large portion of the metrics used to analyze machine learning, we will be using most of the major metrics. Thus, we will be using precision, accuracy, recall and F-measure. Our dataset isn’t particularly large for several reasons. The first reason is we didn’t want to get bogged down too much with data cleaning and checking since the point of the assignment is to focus on Natural Language Processing and not on cleaning data. The other main reason why we chose a smaller dataset is for the sake of computation time. The goal was to not have to wait 5-10 minutes every time we ran the code as this would make debugging more difficult and also potentially grading the assignment more difficult as well. Therefore, since we’re dealing with a dataset which isn’t particularly large, we don’t have high expectations for the outcome of this experiment. If we get an F-score/accuracy of anything reasonably high, we will be happy with that result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-1b729c2fa51d>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-56-1b729c2fa51d>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    pip install --user -U nltk\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# download natural language tool kit\n",
    "# pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk, pandas and scikit-learn and download relevant libraries\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv('deceptive-opinion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"deceptive\"] == 'truthful' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deceptive    0\n",
       "hotel        0\n",
       "polarity     0\n",
       "source       0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text','polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    400\n",
       "negative    400\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    import string as st\n",
    "    temp = [w for w in text if w not in st.punctuation]\n",
    "    return ''.join(temp)\n",
    "df['clean_text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['clean_text','polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"clean_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.remove('not')\n",
    "    temp = [w for w in nltk.word_tokenize(text) if w not in stopwords]\n",
    "    return ' '.join(temp)\n",
    "df['final_text'] = df['clean_text'].apply(delete_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['final_text','polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for x in df['polarity']:\n",
    "    if x == 'positive':\n",
    "        temp.append(1)\n",
    "    else:\n",
    "        temp.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = temp\n",
    "df = df[['final_text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stayed one night getaway family thursday tripl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>triple rate upgrade view room less 200 also in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comes little late im finally catching reviews ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omni chicago really delivers fronts spaciousne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asked high floor away elevator got room pleasa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>booked directly intercontinentala special room...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>good location looks like good property not see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>reading lukewarm reviews hotel went ahead got ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>overview overrated hotel premium location grea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>arrived 130am find sleep not future dance club...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             final_text  sentiment\n",
       "0     stayed one night getaway family thursday tripl...          1\n",
       "1     triple rate upgrade view room less 200 also in...          1\n",
       "2     comes little late im finally catching reviews ...          1\n",
       "3     omni chicago really delivers fronts spaciousne...          1\n",
       "4     asked high floor away elevator got room pleasa...          1\n",
       "...                                                 ...        ...\n",
       "1195  booked directly intercontinentala special room...          0\n",
       "1196  good location looks like good property not see...          0\n",
       "1197  reading lukewarm reviews hotel went ahead got ...          0\n",
       "1198  overview overrated hotel premium location grea...          0\n",
       "1199  arrived 130am find sleep not future dance club...          0\n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectr = TfidfVectorizer(ngram_range=(1,2),min_df=1)\n",
    "vectr.fit(df['final_text'])\n",
    "vect_X = vectr.transform(df['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df['final_text']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(vect_X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78, 10],\n",
       "       [ 1, 71]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Conclusions\n",
    "\n",
    "9. Evaluation\n",
    "    The results of this experiment ended up containing 71 true positive scores , 1 false negative score, 78 true negative scores and 10 false positive scores. In terms of accuracy, our model did quite well. The accuracy ended up being 0.93125 which can be considered exceptional considering the dataset wasn’t particularly large. The recall of this research ended up coming in at 0.98611. Meanwhile, the precision of this experiment ended up scoring 0.87654. Finally, the F-score of this project came out to be 0.92810. Overall, these are exceptional results. Considering the fact that the maximum value for these four metrics is 1.0 and the minimum value is 0.0, we generally consider this research to have been a success. The F-score is the weighted average of both recall and precision and came in above 0.9 as well as the accuracy. This experiment only contained 800 test samples and therefore could be trained to be far more accurate if a larger dataset is used. If a training set of several thousand, or perhaps even several million had been provided, we believe (based on the results of this experiment) that this work could produce exceptionally accurate results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Summary and conclusions\n",
    "\n",
    "    Overall, we found this project to be highly enjoyable. It provided the perfect amount of challenge and the results ended up being surprisingly accurate. This could have significant contributions to many areas of industry, especially the hotel industry. The dataset were raw hotel reviews (some of which were deceptive but those were removed from the dataset) and therefore obviously have applications to that domain. This solution is highly transferable to other areas of research and could potentially be of limitless value. On the other hand, the extent to which this work can be replicated by others is fundamentally limited in nature. The dataset we found was flawless in the sense that there were originally 800 positive reviews and 800 negative reviews and we systematically removed the false positive reviews and the false negative reviews which effectively removed 50% of the dataset. On the programming side, there can’t be too many words said about Python. The libraries in Python have significantly simplified this project. For example, writing the code in order to print the confusion matrix would have most likely been very difficult and time consuming. Programming this project in any other language which lacked the functionality of Python would most likely have been a nightmare. The main benefit of Python is the fact that there are an enormous number of easily accessible libraries in Python which allow the programmer to focus purely on the problem at hand, without having to worry about issues involving syntax. Thus, one potential drawback of someone else using an alternative programming language in order to conduct this experiment is there may not be as many libraries that are as easy to implement as Python contains.\n",
    "\t\n",
    "\tAlso, considering the fact that the dataset we used was rather small and our expectations weren’t especially high for the efficacy of this experiment, the results have been a pleasant surprise. Overall, we consider this project to have been an astounding success and enjoyed it thoroughly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://joannatrojak.medium.com/sentiment-analysis-with-logistic-regression-in-python-with-nltk-library-d5030b1d84e3\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/sentiment-analysis-using-nltk-a-practical-approach/\n",
    "\n",
    "https://www.kaggle.com/rtatman/deceptive-opinion-spam-corpus\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
